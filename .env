# model
#MODEL_ID=toddie314/toddric-zephyr7b
#MODEL_ID=toddie314/toddric-llama-8B-merged-v1
MODEL_ID=toddie314/toddric_v2_merged
#MODEL_ID=toddie314/toddric-3b-merged-v3
#MODEL_ID=Qwen/Qwen2.5-7B-Instruct
# or: MODEL_ID=meta-llama/Llama-3.1-8B-Instruct
#MODEL_ID=meta-llama/Llama-3.1-8B-Instruct
#MODEL_ID=meta-llama/Llama-3.1-8B-Instruct
#MODEL_ID=mistralai/Mistral-7B-v0.1
#MODEL_DIR=./models/toddric-llama-8B-merged-v1
MODEL_DIR=./models/toddie314/toddric_v2_merged

#transcripts
LOG_TRANSCRIPTS=1
LOG_DIR=./logs
LOG_ROTATE_MB=50
LOG_LEVEL=INFO

# auth
ALLOW_NO_AUTH=1   # for local dev; unset in prod
# TODDRIC_BEARER=supersecret

# prefer a file; fallback to env string
SYSTEM_PROMPT_FILE=prompts/system_toddric.md
# optional: if set, overrides the file
# SYSTEM_PROMPT=

# quant / device
#LOAD_IN_4BIT=1
BITS=4
LOW_CPU_MEM=0
DEVICE_MAP=auto
TORCH_DTYPE=bfloat16
TORCH_DEVICE=auto

# decoding defaults (tight + helpful)
DO_SAMPLE=0
REPETITION_PENALTY=1.12

# perf
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
RECIPE_AUTODETECT=1
TEMPERATURE=0
TOP_P=1
TOP_K=50
MAX_NEW_TOKENS=192
# use PyTorch SDPA attention kernels
ATTN_IMPL=sdpa
# fast & safe on your GPU
DTYPE=bfloat16
WARMUP=0
QUANT=4bit
